# Configurations to accelerate data packing/padding in training
training:

  # attention module configurations
  # e.g. padding-free modifications to attention layer
  attention:

    # this controls the confgurations for padding_free computation of attention
    padding_free: 
      method: "huggingface"
